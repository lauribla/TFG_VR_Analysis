"""
VR USER EVALUATION - An√°lisis completo de la base de datos MongoDB
------------------------------------------------------------------
Este script conecta con la base de datos donde Unity guarda los logs (test.tfg),
los analiza y genera autom√°ticamente:
    - M√©tricas por categor√≠a (efectividad, eficiencia, satisfacci√≥n, presencia)
    - M√©tricas globales ponderadas
    - Archivos CSV/JSON exportados
    - Gr√°ficas comparativas
    - Informe PDF con los resultados
"""

import pandas as pd
import shutil
from python_analysis.log_parser import LogParser
from python_analysis.metrics import MetricsCalculator
from python_analysis.exporter import MetricsExporter
from python_visualization.visualize_groups import Visualizer
from python_visualization.spatial_plotter import SpatialVisualizer
from python_visualization.pdf_reporter import PDFReport
from datetime import datetime
import os
import json
from pathlib import Path

# ============================================================
# 1Ô∏è‚É£ Conectar con MongoDB y cargar logs
# ============================================================

# Conectando con par√°metros del .env (gesti√≥n autom√°tica en LogParser)
parser = LogParser()
print(f"üîó Conectando a MongoDB ‚Üí URI: {parser.mongo_uri} | DB: {parser.db_name} | COL: {parser.collection_name}")
logs = parser.fetch_logs()

# df sin expandir ‚Üí recuperar config
df_raw = parser.parse_logs(logs, expand_context=False)

# df expandido ‚Üí m√©tricas
df = parser.parse_logs(logs, expand_context=True)

# Buscar cuestionarios (SUS) antes de cerrar conexi√≥n
quest_data = []
try:
    quests_col = parser.client[parser.db_name]["questionnaires"]
    quest_data = list(quests_col.find({}))
except Exception as e:
    print(f"‚ö†Ô∏è Warning: Podr√≠a no haber cuestionarios. {e}")

parser.close()

if df.empty:
    print("‚ö†Ô∏è  No se encontraron logs en Mongo.")
    exit()

print(f"‚úÖ {len(df)} documentos cargados desde Mongo.\n")


# ============================================================
# 2Ô∏è‚É£ Extraer config (Log vs Local override)
# ============================================================

print("‚öôÔ∏è  Leyendo configuraci√≥n del experimento...\n")

experiment_config = None

# Check override
if os.environ.get("FORCE_LOCAL_CONFIG", "false").lower() == "true":
    config_path = Path("vr_logger/experiment_config.json")
    if config_path.exists():
        print(f"‚ö†Ô∏è  FORZANDO CONFIGURACI√ìN LOCAL: {config_path}")
        with open(config_path, "r", encoding="utf-8") as f:
            experiment_config = json.load(f)
    else:
        print(f"‚ùå  No se encontr√≥ la configuraci√≥n local en {config_path}")

# Fallback/Default: Extract from logs
if experiment_config is None:
    configs = [entry for entry in logs if entry.get("event_type") == "config"]
    if configs:
        # Ordenar por timestamp para asegurar que usamos la √öLTIMA (m√°s reciente)
        # Asumimos que timestamp es comparable (datetime o string ISO)
        try:
            configs.sort(key=lambda x: x.get("timestamp", ""))
            latest_config_log = configs[-1]
            experiment_config = latest_config_log.get("event_context")
            print(f"‚úÖ Configuraci√≥n cargada desde logs (La m√°s reciente: {latest_config_log.get('timestamp')})")
        except Exception as e:
            print(f"‚ö†Ô∏è Error ordenando configs, usando la √∫ltima encontrada: {e}")
            experiment_config = configs[-1].get("event_context")

if experiment_config is not None:
    print("‚úÖ Config cargada correctamente.\n")

    # ------------------------------------------------------------
    # FILTRADO POR SESSION_NAME
    # ------------------------------------------------------------
    # Objetivo: Analizar SOLO las sesiones que coincidan con el session_name del config actual
    # para evitar mezclar experimentos distintos (ej: "Experiment_A" vs "Experiment_B")

    target_session_name = experiment_config.get("session", {}).get("session_name")

    if target_session_name:
        print(f"üéØ Target Session Name: '{target_session_name}' (from config)")

        # Estrategia: Buscar en los logs 'config' o 'session_start' qu√© session_ids tienen este nombre
        # IMPORTANTE: Usamos df_raw porque tiene la columna 'context' como string JSON.
        # df (el expandido) NO tiene 'context' porque lo expandi√≥ en columnas.

        # 1. Buscar en configs (usando df_raw)
        matching_configs = []
        if "context" in df_raw.columns:
             matching_configs = df_raw[
                (df_raw["event_type"] == "config") &
                (df_raw["context"].fillna("").apply(lambda x: target_session_name in str(x)))
            ]["session_id"].unique()

        # 2. Buscar en session_start (usando df_raw)
        matching_starts = []
        if "context" in df_raw.columns:
            matching_starts = df_raw[
                (df_raw["event_type"] == "session_start") &
                (df_raw["context"].fillna("").apply(lambda x: target_session_name in str(x)))
            ]["session_id"].unique()

        # 3. Fallback: Si df_raw no funciona, buscar en df expandido (columna 'session' si existe)
        matching_expanded = []
        if "session" in df.columns: # A veces 'session' se expande como columna si el json lo ten√≠a
             matching_expanded = df[
                df["session"].apply(lambda x: isinstance(x, dict) and x.get("session_name") == target_session_name if isinstance(x, dict) else False)
             ]["session_id"].unique()
        elif "session_session_name" in df.columns: # Flattened key pattern
             matching_expanded = df[df["session_session_name"] == target_session_name]["session_id"].unique()


        # Unir todos los session_ids v√°lidos
        valid_sessions = set(matching_configs) | set(matching_starts) | set(matching_expanded)

        # Si no encontramos nada con esos m√©todos, intentamos mirar si el propio config actual tiene session_id
        current_config_sid = experiment_config.get("session_id")
        if current_config_sid:
            valid_sessions.add(current_config_sid)

        if valid_sessions:
            original_count = df["session_id"].nunique()
            df = df[df["session_id"].isin(valid_sessions)]
            filtered_count = df["session_id"].nunique()
            print(f"üßπ Filtrando logs... Se mantienen {filtered_count} sesiones de {original_count} totales.\n")
        else:
            print(f"‚ö†Ô∏è No se encontraron sesiones coincidiendo con '{target_session_name}' en los logs. Mostrando todo.\n")
    else:
        print("‚ö†Ô∏è El config no tiene 'session_name'. No se puede filtrar por experimento.\n")

else:
    print("‚ö†Ô∏è  No existe configuraci√≥n en los logs y no se forz√≥ local.\n")


# ============================================================
# 3Ô∏è‚É£ Resumen de sesiones y usuarios
# ============================================================

print("üë• Resumen de usuarios, grupos y sesiones:")

usuarios = df["user_id"].nunique()
grupos = df["group_id"].nunique()
sesiones = df["session_id"].nunique()

print(f"  ‚Ä¢ Usuarios: {usuarios}")
print(f"  ‚Ä¢ Grupos: {grupos}")
print(f"  ‚Ä¢ Sesiones: {sesiones}\n")

print("üìÑ Lista de sesiones detectadas:")
print(df[["user_id", "group_id", "session_id"]].drop_duplicates().to_string(index=False))


# ============================================================
# 4Ô∏è‚É£ Calcular m√©tricas usando MetricsCalculator
# ============================================================

print("\nüìä Calculando m√©tricas ponderadas del experimento...\n")

metrics = MetricsCalculator(df, experiment_config=experiment_config)
raw_results = metrics.compute_all()

# ------------------------------------------------------------
# ADAPTAR RESULTADO a FORMATO PARA EL PDF Y EXPORTER
# ------------------------------------------------------------
results_for_export = {}

for categoria, contenido in raw_results["categorias"].items():

    # Subestructura compatible con PDFReporter
    results_for_export[categoria] = {
        "score": contenido["score"]
    }

    for metric_name, metric_data in contenido.items():
        if isinstance(metric_data, dict):
            results_for_export[categoria][metric_name] = metric_data["raw"]

# a√±adir puntuaci√≥n global
results_for_export["global_score"] = raw_results["global_score"]

print(json.dumps(results_for_export, indent=4))


# ============================================================
# 5Ô∏è‚É£ Crear carpetas de exportaci√≥n UNIFICADAS
# ============================================================

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

base_dir = Path(__file__).parent / "pruebas"
output_dir = base_dir / f"analysis_{timestamp}"
results_dir = output_dir / "results"
figures_dir = output_dir / "figures"

# Crear estructura
os.makedirs(results_dir, exist_ok=True)
os.makedirs(figures_dir, exist_ok=True)

print(f"üìÇ Carpeta de salida creada: {output_dir}")


# ============================================================
# 6Ô∏è‚É£ Guardar config en archivo
# ============================================================

if experiment_config is not None:
    config_path = results_dir / "experiment_config_from_mongo.json"
    with open(config_path, "w", encoding="utf-8") as f:
        json.dump(experiment_config, f, indent=4)
    print(f"üìÑ Config exportada: {config_path.name}\n")


# ============================================================
# 7Ô∏è‚É£ Exportar resultados JSON + CSV
# ============================================================

print("üíæ Exportando m√©tricas...")

exporter = MetricsExporter(results_for_export, output_dir=results_dir)
exporter.to_json("results.json")
exporter.to_csv("results.csv")

grouped_df = metrics.compute_grouped_metrics()

# --- INTEGRATING SUBJECTIVE QUESTIONNAIRES ---
print("üìã Integrando cuestionarios subjetivos (SUS)...")
if quest_data and not grouped_df.empty:
    df_q = pd.DataFrame(quest_data)
    cols_to_keep = ["user_id", "sus_score", "subj_efectividad", "subj_eficiencia", "subj_satisfaccion", "subj_presencia"]
    cols_to_keep = [c for c in cols_to_keep if c in df_q.columns]
    
    if len(cols_to_keep) > 1: # user_id + al menos otra columna
        df_q = df_q[cols_to_keep]
        # Quedarse con el √∫ltimo intento en caso de duplicados
        df_q = df_q.drop_duplicates(subset=["user_id"], keep="last")
        grouped_df = pd.merge(grouped_df, df_q, on="user_id", how="left")
        print(f"‚úÖ Se cruzaron datos subjetivos (SUS) correctamente.")
else:
    print("‚ö†Ô∏è No hay datos de cuestionarios para cruzar.")
# ---------------------------------------------

grouped_path = results_dir / "grouped_metrics.csv"
grouped_df.to_csv(grouped_path, index=False)

# Tambi√©n exportar versi√≥n agrupada como JSON
MetricsExporter.export_multiple(
    [results_for_export],
    ["Global"],
    mode="json",
    output_dir=results_dir,
    filename="group_results"
)

print("‚úÖ Exportaci√≥n completada.\n")


# ============================================================
# 8Ô∏è‚É£ Generar figuras
# ============================================================

print("üìà Generando gr√°ficas...")

global_json = results_dir / "group_results.json"
generated_figures = 0

if global_json.exists():
    global_dir = figures_dir / "global"
    viz_global = Visualizer(str(global_json), output_dir=global_dir)
    viz_global.generate_all()
    generated_figures += len(list(global_dir.glob("*.png")))

if grouped_path.exists():
    grouped_dir = figures_dir / "agrupado"
    viz_grouped = Visualizer(str(grouped_path), output_dir=grouped_dir)
    viz_grouped.generate_all()
    generated_figures += len(list(grouped_dir.glob("*.png")))

    generated_figures += len(list(grouped_dir.glob("*.png")))

print(f"üìä Figuras generadas: {generated_figures}\n")

# ============================================================
# 8.1Ô∏è‚É£  Generar figuras espaciales (Mapas de calor / Trayectorias)
# ============================================================
print("üó∫Ô∏è Generando visualizaciones espaciales (si existen datos de tracking)...")
play_area_w = None
play_area_d = None
if experiment_config is not None:
    play_area_w = experiment_config.get("session", {}).get("play_area_width")
    play_area_d = experiment_config.get("session", {}).get("play_area_depth")

spatial_viz = SpatialVisualizer(
    df, 
    output_dir=figures_dir / "spatial",
    play_area_width=play_area_w,
    play_area_depth=play_area_d
)
spatial_viz.generate_all()
print("\n")


# ============================================================
# 9Ô∏è‚É£ Generar informes PDF
# ============================================================

print("üìÑ Generando informe PDF...\n")

# Usar el path consolidado 'output_dir'
pdf_output_path = output_dir / "final_report.pdf"

# Priorizamos 'agrupado' para el reporte si existe, ya que es m√°s completo para gr√°ficas
report_file = grouped_path if grouped_path.exists() else global_json

if report_file.exists():
    report = PDFReport(
        results_file=str(report_file),
        figures_dir=figures_dir,  # Pasamos la ra√≠z de figuras
        output_dir=output_dir     # Pasamos la ra√≠z de output
    )
    report.generate()

print("üéâ AN√ÅLISIS COMPLETO FINALIZADO.\n")
